# -*- coding: utf-8 -*-
"""loan-prediction-ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZdhmGHv7lEEQY00dgclEeZuNszfeupfn

# Introduction

> ### Steps are:

1. [Gathering Data](#1)
2. [Exploratory Data Analysis](#2)
3.  [Data Visualizations](#3)
4.  [Machine Learning Model Decision.](#4)
5.  [Traing the ML Model](#5)
6. [Predict Model](#6)

# Import Packages
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

"""You can load this data with the `read_csv()` method from `pandas` package. It converts the data set to a python dataframe.

## Dataset Key Information.
  
---  
  
> - `Loan_ID`--------------> Unique Loan ID.
- `Gender`   --------------> Male/ Female 
- `Married`  --------------> Applicant married (Y/N) 
- `Dependents` ------------> Number of dependents 
- `Education` -------------> Applicant Education (Graduate/ Under Graduate) 
- `Self_Employed` ---------> Self-employed (Y/N) 
- `ApplicantIncome` -------> Applicant income 
- `CoapplicantIncome` -----> Coapplicant income 
- `LoanAmount`  -----------> Loan amount in thousands 
- `Loan_Amount_Term` ------> Term of a loan in months 
- `Credit_History` --------> Credit history meets guidelines 
- `Property_Area` ---------> Urban/ Semi-Urban/ Rural 
- `Loan_Status` -----------> Loan approved (Y/N)

<a id="1"></a><br>
# 1. Gathering Data
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive/')
# %cd /gdrive

ls

cd/gdrive/My Drive/Loan Prediction/

ls

#  Create New Variable and stores the dataset values as Data Frame
loan_train = pd.read_csv('/gdrive/My Drive/Loan Prediction/loan-train.csv')
loan_test = pd.read_csv('/gdrive/My Drive/Loan Prediction/loan-test.csv')

"""- Lets display the some few information from our large datasets

Here, We shows the first five rows from datasets
"""

loan_train.head()

"""- As we can see in the above output, there are too many columns, ( columns known as features as well. )

We can also use `loan_train` to show few rows from the first five and last five record from the dataset
"""

loan_train

"""> ### Here, we can see there are many rows and many columns, To know how many records and columns are available in our dataset, we can use the `shape` attribute or we can use `len()` to know how many records and how many features available in the dataset."""

print("Rows: ", len(loan_train))

"""Pandas has inbuild attribute to get all column from the dataset, With the help of this feature we can get the how many column available we have."""

print("Columns: ", len(loan_train.columns))

"""Also we can get the shape of the dataset using `shape` attribute"""

print("Shape : ", loan_train.shape)

"""> ### *After we collecting the data, Next step we need to understand what kind of data we have.*

### Also we can get the column as an list(array) from dataset

> **Note: DataFrame.columns returns the total columns of the dataset,
> Store the number of columns in variable `loan_train_columns`**
"""

loan_train_columns = loan_train.columns # assign to a variable
loan_train_columns # print the list of columns

"""### Now, Understanding the Data

- First of all we use the `loan_train.describe()` method to shows the important information from the dataset
- It provides the `count`, `mean`, `standard deviation (std)`, `min`, `quartiles` and `max` in its output.
"""

loan_train.describe()

"""#### As I said the above cell, this the information of all the methamatical details from dataset. Like `count`, `mean`, `standard deviation (std)`, `min`, `quartiles(25%, 50%, 75%)` and `max`.

> ### Another method is `info()`, This method show us the information about the dataset, Like

1. What's the type of culumn have?
- How many rows available in the dataset?
- What are the features are there?
- How many null values available in the dataset?
- Ans so on...
"""

loan_train.info()

"""As we can see in the output.

1. There are `614` entries
- There are total 13 features (0 to 12)
- There are three types of datatype `dtypes: float64(4), int64(1), object(8)`
- It's Memory usage that is, `memory usage: 62.5+ KB`
- Also, We can check how many missing values available in the `Non-Null Count` column

<a id="2"></a><br>
# 2. Exploratory Data Analysis

In this section, We learn about extra information about data and it's characteristics.

- First of all, We explore object type of data
So let's make a function to know how many types of values available in the column
"""

def explore_object_type(df ,feature_name):
    """
    To know, How many values available in object('categorical') type of features
    And Return Categorical values with Count.
    """    
    if df[feature_name].dtype ==  'object':
        print(df[feature_name].value_counts())

"""- After defined a function, Let's call it. and check what's the output of our created function."""

# Now, Test and Call a function for gender only
explore_object_type(loan_train, 'Gender')

"""- Here's one little issue occurred, Suppose in your datasets there are lots of feature to defined like this above code. """

# Solution is, Do you remember we have variable with name of `loan_train_columns`, Right,  let's use it
# 'Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status'

for featureName in loan_train_columns:
    if loan_train[featureName].dtype == 'object':
        print('\n"' + str(featureName) + '\'s" Values with count are :')
        explore_object_type(loan_train, str(featureName))

"""> ## *Note: Your output maybe shorter or longer, It's totally depend upon your dataset's columns*

- We need to fill null values with `mean` and `median` using `missingno` package
"""

import missingno as msno

# list of how many percentage values are missing
loan_train

loan_train.isna().sum()
# round((loan_train.isna().sum() / len(loan_train)) * 100, 2)

msno.bar(loan_train)

msno.matrix(loan_train )

"""- As we can see here, there are too many columns missing with small amount of null values so we use `mean` amd `mode` to replace with `NaN` values."""

loan_train['Credit_History'].fillna(loan_train['Credit_History'].mode(), inplace=True) # Mode
loan_test['Credit_History'].fillna(loan_test['Credit_History'].mode(), inplace=True) # Mode


loan_train['LoanAmount'].fillna(loan_train['LoanAmount'].mean(), inplace=True) # Mean
loan_test['LoanAmount'].fillna(loan_test['LoanAmount'].mean(), inplace=True) # Mean

"""### # convert Categorical variable with Numerical values.

`Loan_Status` feature boolean values, So we replace `Y` values with `1` and `N` values with `0`
and same for other `Boolean` types of columns
"""

loan_train.Loan_Status = loan_train.Loan_Status.replace({"Y": 1, "N" : 0})
# loan_test.Loan_Status = loan_test.Loan_Status.replace({"Y": 1, "N" : 0}) 

loan_train.Gender = loan_train.Gender.replace({"Male": 1, "Female" : 0})
loan_test.Gender = loan_test.Gender.replace({"Male": 1, "Female" : 0})

loan_train.Married = loan_train.Married.replace({"Yes": 1, "No" : 0})
loan_test.Married = loan_test.Married.replace({"Yes": 1, "No" : 0})

loan_train.Self_Employed = loan_train.Self_Employed.replace({"Yes": 1, "No" : 0})
loan_test.Self_Employed = loan_test.Self_Employed.replace({"Yes": 1, "No" : 0})

loan_train['Gender'].fillna(loan_train['Gender'].mode()[0], inplace=True)
loan_test['Gender'].fillna(loan_test['Gender'].mode()[0], inplace=True)

loan_train['Dependents'].fillna(loan_train['Dependents'].mode()[0], inplace=True)
loan_test['Dependents'].fillna(loan_test['Dependents'].mode()[0], inplace=True)

loan_train['Married'].fillna(loan_train['Married'].mode()[0], inplace=True)
loan_test['Married'].fillna(loan_test['Married'].mode()[0], inplace=True)

loan_train['Credit_History'].fillna(loan_train['Credit_History'].mean(), inplace=True)
loan_test['Credit_History'].fillna(loan_test['Credit_History'].mean(), inplace=True)

"""* Here, `Property_Area`, `Dependents` and `Education` has multiple values so now we can use `LabelEncoder` from `sklearn` package"""

from sklearn.preprocessing import LabelEncoder
feature_col = ['Property_Area','Education', 'Dependents']
le = LabelEncoder()
for col in feature_col:
    loan_train[col] = le.fit_transform(loan_train[col])
    loan_test[col] = le.fit_transform(loan_test[col])

"""> ### Finally, We have all the features with numerical values,

<a id="3"></a><br>
# 3. Data Visualizations


In this section, We are showing the visual information from the dataset, For that we need some pakages that are `matplotlib` and `seaborn`
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline


import seaborn as sns
sns.set_style('dark')

loan_train

loan_train.plot(figsize=(18, 8))

plt.show()

plt.figure(figsize=(18, 6))
plt.subplot(1, 2, 1)


loan_train['ApplicantIncome'].hist(bins=10)
plt.title("Loan Application Amount ")

plt.subplot(1, 2, 2)
plt.grid()
plt.hist(np.log(loan_train['LoanAmount']))
plt.title("Log Loan Application Amount ")

plt.show()

plt.figure(figsize=(18, 6))
plt.title("Relation Between Applicatoin Income vs Loan Amount ")

plt.grid()
plt.scatter(loan_train['ApplicantIncome'] , loan_train['LoanAmount'], c='k', marker='x')
plt.xlabel("Applicant Income")
plt.ylabel("Loan Amount")
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(loan_train['Loan_Status'], loan_train['LoanAmount'])
plt.title("Loan Application Amount ")
plt.show()

plt.figure(figsize=(12,8))
sns.heatmap(loan_train.corr(), cmap='coolwarm', annot=True, fmt='.1f', linewidths=.1)
plt.show()

"""In this heatmap, we can clearly seen the relation between two variables

<a id="4"></a><br>
# 4. Choose ML Model.

* In this step, We have a lots of Machine Learning Model from sklearn package, and we need to decide which model is give us the better performance. then we use that model in final stage and send to the production level.
"""

# import ml model from sklearn pacakge

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score

"""First of all, we are use `LogisticRegression` from `sklearn.linear_model` package. Here is the little information about `LogisticRegression`.

`Logistic Regression` is a **classification algorithm**. It is used to predict a binary outcome (`1 / 0`, `Yes / No`, and `True / False`) given a set of independent variables. To represent binary / categorical outcome, we use dummy variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as the dependent variable.

![](https://www.analyticsvidhya.com/wp-content/uploads/2015/10/logit.png)

* Let's build the model
"""

logistic_model = LogisticRegression()

"""<a id="5"></a><br>
# 5. Traing the ML Model

> ### **Before fitting the model, We need to decide how many feature are available for testing and training, then after complete this step. fitt the model** 

Currently, we are using `Credit_History', 'Education', 'Gender` features for training so let's create train and test variables
"""

train_features = ['Credit_History', 'Education', 'Gender']

x_train = loan_train[train_features].values
y_train = loan_train['Loan_Status'].values

x_test = loan_test[train_features].values

logistic_model.fit(x_train, y_train)



"""<a id="6"></a><br>
# 6. Predict Model
"""

# Predict the model for testin data

predicted = logistic_model.predict(x_test)

# check the coefficeints of the trained model
print('Coefficient of model :', logistic_model.coef_)

# check the intercept of the model
print('Intercept of model',logistic_model.intercept_)

# Accuray Score on train dataset
# accuracy_train = accuracy_score(x_test, predicted)
score = logistic_model.score(x_train, y_train)
print('accuracy_score overall :', score)
print('accuracy_score percent :', round(score*100,2))

# predict the target on the test dataset
predict_test = logistic_model.predict(x_test)
print('Target on test data',predict_test)